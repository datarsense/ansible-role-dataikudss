---
- name: Converge
  hosts: all
  vars:
    skip_handlers: true
    dss_backend_xmx: 2g
    dss_jek_xmx: 2g
    dss_fek_xmx: 2g
    
    configure_mysql: true
    mysql_jdbc_connector_url: https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31.jar

    configure_ldap_settings: true
    ldap_url: "ldap://ldap.internal.example.com/dc=example,dc=com"
    ldap_binddn: "uid=readonly,ou=users,dc=example,dc=com"
    ldap_bindpassword: ""
    ldap_usetls: true
    ldap_autoimportusers: true
    ldap_userfilter: "(&(objectClass=posixAccount)(uid={USERNAME}))"
    ldap_defaultuserprofile: "READER"
    ldap_displaynameattribute: "cn"
    ldap_emailattribute: "mail"
    ldap_enablegroups: true
    ldap_groupfilter: "(&(objectClass=posixGroup)(memberUid={USERDN}))"
    ldap_groupnameattribute: "cn"
    ldap_groupprofiles: []
    ldap_authorizedgroups: "dss-users"

    configure_oidc_sso: true
    oidc_clientid: test
    oidc_clientsecret: test
    oidc_scope: 'openid profile email'
    oidc_issuer: https://accounts.google.com
    oidc_authorizationendpoint: https://accounts.google.com/o/oauth2/v2/auth
    oidc_tokenendpoint: https://oauth2.googleapis.com/token
    oidc_jwksuri: https://www.googleapis.com/oauth2/v3/certs
    oidc_claimkeyidentifier: email_verified

    configure_spark: true
    spark_executionconfigs:
      - name": SparkOnKubernetes
        kubernetesSettings:
          managedKubernetes: true
          managedNamespace: testnamespace
          authenticationMode: BUILTIN
          ensureNamespaceCompliance: false
          createNamespace: false
          baseImageType: SPARK
          baseImage: dss_spark_base:latest
          repositoryURL: docker.io
          prePushMode: NONE
          dockerTLSVerify: false
    configure_k8s: true
    download_dss_docker_images: true
    download_dss_docker_images_url_tmp_directory: "{{ dss_service_user_home_basedir }}/{{ dss_service_user }}/.tmp"
    k8s_executionconfigs:
      - name: test1
        type: KUBERNETES
        properties: []
        usableBy: ALLOWED
        allowedGroups:
          - administrators
        dockerNetwork: host
        dockerResources: []
        kubernetesNamespace: testnamespace
        kubernetesResources:
          memRequestMB: 2048
          memLimitMB: 2048
          cpuRequest: 2.0
          cpuLimit: 2.0
          customLimits: []
          customRequests: []
        hostPathVolumes: []
        isFinal: false,
        ensureNamespaceCompliance: false
        createNamespace: false
        baseImageType: EXEC
        baseImage: dss_containerer_exec_base:latest
        repositoryURL: docker.io
        prePushMode: NONE
        dockerTLSVerify: false
    configure_uif: true
    uif_users:
      userA:
        group: groupA
      userB:
        group: groupB
    uif_userrules:
      - name: rule1
        scope: GLOBAL
        type: SINGLE_MAPPING
        dssUser: userA
        targetUnix: unix-userA
        targetHadoop: hadoop-userA
      - name: rule2
        scope: GLOBAL
        type: REGEXP_RULE
        ruleFrom: .*
        targetUnix: unix-userB
        targetHadoop: hadoop-userB
    uif_grouprules:
      - name: ruleGroupA
        scope: GLOBAL
        type: SINGLE_MAPPING
        dssGroup: groupA
        targetUnix: unix-userA
        targetHadoop: hadoop-userA
      - name: ruleGroupB
        scope: GLOBAL
        type: REGEXP_RULE
        ruleFrom: .*
        targetUnix: unix-userB
        targetHadoop: hadoop-userB
  # https://www.jeffgeerling.com/blog/2020/resolving-fedora-dnf-error-no-such-file-or-directory-varlibdnfrpmdblockpid
  pre_tasks:
    - name: Wait for systemd to complete initialization.  # noqa 303
      ansible.builtin.command: systemctl is-system-running
      register: systemctl_status
      until: >
        'running' in systemctl_status.stdout or
        'degraded' in systemctl_status.stdout
      retries: 30
      delay: 5
      when: ansible_service_mgr == 'systemd'
      changed_when: false
      failed_when: systemctl_status.rc > 1
    - name: "Update Repository cache and install gnupg (required by apt-key)"
      ansible.builtin.apt:
        name: "{{ packages }}"
        state: present
        update_cache: true
      vars:
        packages:
          - gnupg
          - python3.7
          - python3-distutils
          - python3-pip
          - python3-selinux
          - python3-setuptools
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
    - name: "Deploy docker"
      ansible.builtin.include_role:
        name: geerlingguy.docker
      vars:
        docker_service_enabled: false
        docker_service_state: stopped
      tags: [docker-images, molecule-idempotence-notest]
    - name: "Deploy docker python library"
      ansible.builtin.include_role:
        name: geerlingguy.pip
      tags: [docker-images]
  tasks:
    - name: "Test DSS install"
      ansible.builtin.include_role:
        name: "{{ lookup('env', 'MOLECULE_PROJECT_DIRECTORY') | basename }}"
      vars:
        dss_version: "10.0.7"
        dss_hadoop_package: "dataiku-dss-hadoop-standalone-libs-generic-hadoop3-10.0.7.tar.gz"
        dss_spark_package: "dataiku-dss-spark-standalone-10.0.7-3.1.2-generic-hadoop3.tar.gz"
    - name: "Test DSS upgrade"
      ansible.builtin.include_role:
        name: "{{ lookup('env', 'MOLECULE_PROJECT_DIRECTORY') | basename }}"
      vars:
        dss_version: "11.1.1"
        dss_hadoop_package: "dataiku-dss-hadoop-standalone-libs-generic-hadoop3-11.1.1.tar.gz"
        dss_spark_package: "dataiku-dss-spark-standalone-11.1.1-3.2.1-generic-hadoop3.tar.gz"