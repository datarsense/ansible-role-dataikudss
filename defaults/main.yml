---
# Defaults variables for ansible-role-dss

# Settings for DSS deployment. 
# License file is not provided by the role. The dss_license_file variable 
# has to be changed according to the license.json file location on the host executing ansible-playbok
dss_base_repository_url: https://cdn.downloads.dataiku.com/public/studio
dss_version: 12.1.0
dataiku_python_api_package: dataiku-api-client
dss_api_version: 12.1.0
dss_service_user: dataiku
dss_service_user_shell: "/bin/bash"
dss_service_user_home_basedir: /home
dss_install_dir_location: /opt/dataiku
dss_node_poll_fqdn: true # If true, use ansible_fqdn else use ansible_host
dss_license_file: license.json
dss_node_type: design
dss_datadir: dss_data
dss_network_port: 10000

# Optional : add Mysql JDBC connector
configure_mysql: false

# Optional : add containerized execution on kubernetes capability
configure_k8s: false
k8s_executionconfigs: []
download_dss_docker_images: false
download_dss_docker_images_url: "{{ dss_base_repository_url }}/{{ dss_version }}/container-images/dataiku-dss-ALL-base_dss-{{ dss_version }}-r-py3.6.tar.gz"

# Optional : add dss spark support
configure_spark: true
dss_hadoop_package: "dataiku-dss-hadoop-standalone-libs-generic-hadoop3-12.1.0.tar.gz"
dss_spark_package: "dataiku-dss-spark-standalone-12.1.0-3.3.1-generic-hadoop3.tar.gz"
spark_executionconfigs:
  - name: default
    description: |- 
      This default configuration sets a few parameters that are suitable for a wide range of use cases.
      Importantly in order to work in all circumstances it does not set the spark master configuration.
      It will thus use the master defined by your default Spark configuration.
      This may lead Spark jobs to execute locally without using your cluster. You may need for example to add spark.master\u003dyarn-client
    conf:
      - key: spark.executor.memory
        value: 2400m
        isFinal: false
        secret: false
      - key: spark.sql.shuffle.partitions
        value: 40
        isFinal: false
        secret: false
      - key: spark.yarn.executor.memoryOverhead
        value: 600
        isFinal: false
        secret: false
      - key: spark.port.maxRetries
        value: 200
        isFinal: false
        secret: false
  - name: sample-yarn-config
    description: |- 
      This sample configuration shows a possible set of parameters for running DSS Spark jobs on YARN.
      These settings are suitable for a small cluster.
      You will need to tune spark.executor.instances spark.executor.cores and memory settings based on the size of your YARN cluster.
    conf:
      - key: spark.master
        value: yarn-client
        isFinal: false
        secret: false
      - key: spark.executor.memory
        value: 4g
        isFinal: false
        secret: false
      - key: spark.executor.instances
        value: 4
        isFinal: false
        secret: false
      - key: spark.executor.cores
        value: 2
        isFinal: false
        secret: false
      - key: spark.sql.shuffle.partitions
        value: 40
        isFinal: false
        secret: false
      - key: spark.yarn.executor.memoryOverhead
        value: 1200
        isFinal: false
        secret: false
      - key: spark.port.maxRetries
        value: 200
        isFinal: false
        secret: false      
  - name: sample-local-config
    description: |-
      This sample configuration shows a possible set of parameters for running DSS Spark jobs locally (non distributed).
      This can be useful for testing on small jobs as local Spark jobs start faster than YARN ones but is not suitable for production usage.
    conf:
      - key: spark.master
        value: local[4]
        isFinal: false
        secret: false  
      - key: spark.driver.memory
        value: 3g
        isFinal: false
        secret: false
      - key: spark.sql.shuffle.partitions
        value: 40
        isFinal: false
        secret: false
      - key: spark.port.maxRetries
        value: 200
        isFinal: false
        secret: false  

# Optional : Activate LDAP authentication
configure_ldap_settings: false

# Optional : OpenID Connect (OIDC) Single Sign-On authentication
configure_oidc_sso: false
oidc_scope: 'openid email'

# Optional : Activate User Isolation Framework (UIF)
configure_uif: false
uif_users: {}
uif_userrules: []
uif_grouprules: []